---
title: "Heart Attack Evaluation"
author: 'Group 7: Megan Lin, Eva Mustafic, James Powell'
date: "4/14/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(caret)
```

Part 1. Select either as a lab or individual two datasets that you have not used before but that are of interest to you/group. Define questions that can be answered using a classification, specifically kNN, for each dataset. Build kNN models and then use the evaluation metrics we discussed in class (Accuracy, TPR, FPR, F1, Kappa, LogLoss and ROC/AUC) to assess the quality of the models. Make sure to calculate the base rate or prevalence to provide a reference for some of these measures.

Data Source: https://www.kaggle.com/rashikrahmanpritom/heart-attack-analysis-prediction-dataset

```{r echo = FALSE}
#Let's load in our data
data <- read_csv('heart.csv')
#No missing datapoints so no need to clean

#Recode gender from 0->female, 1->male
data$sex <- recode(data$sex, '0' = 'female', '1' = 'male')

#Recode fasting blood sugar>120 from 0->low, 1->high
data$fbs <- recode(data$fbs, '0' = 'Low', '1' = 'High')

#Recode exercise induced angina from 0->no, 1->yes
data$exng <- recode(data$exng, '0' = 'No', '1' = 'Yes')

#Recode chest pain to the types of chest pain
data$cp <- recode(data$cp, '0' = 'typ angina', '1' = 'atyp angina', '2' = 'non-ang pain', '3'='asymptomatic')

#Recode resting electrocardiographic results  from 0->normal, 1->ST-T wave, 2->ventricular hypertrophy'
data$restecg <- recode(data$restecg, '0' = 'normal', '1' = 'ST-T', '2' = 'ventr hypertrophy')

#Recode the columns to be numeric
data$age <- as.numeric(data$age)
data$trtbps <- as.numeric(data$trtbps)
data$chol <- as.numeric(data$chol)
data$thalachh <- as.numeric(data$thalachh)
data$oldpeak <- as.numeric(data$oldpeak)

```


Part 2. Take a closer look at where miss-classification errors are occurring, is there a pattern? If so discuss this pattern and why you think this is the case. 

```{r echo = FALSE}

pred <- data
set.seed(1980)

split_index <- createDataPartition(pred$output, p = .8,
                                  list = FALSE,
                                  times = 1)

train <- pred[split_index,]
test <- pred[-split_index,]

tree <- train(output~., #model formula everything used to classify output
                   data=train, #use the training data
                   method='rpart',# indicates the use of tree based model
                   na.action = na.omit)#omitting the missing values
                   
tree #The final value for the cp is 0.0603 based off of 243 samples and 13 predictors

#Quick overview of how bootstrapping works. 
xx <- tibble(tree$resample)
mean(xx$Accuracy)

tree$finalModel$variable.importance

#Based off of this evaluation, angina is the most important variable when approaching focusing on which variable will reduce model error

#And now just for fun! A barplot of the level of importance each variable plays
library(RColorBrewer)
coul <- brewer.pal(5, "Set2")
barplot(tree$finalModel$variable.importance, col=coul)

```

```{r echo = FALSE}
#First we need to do some predictions using the test data 

eval <-(predict(tree,newdata = test))#generates 1s and 0s

eval_prob <- predict(tree, newdata = test, type = "prob")#this gives us the predicted prob, which we will use later for the fairness evaluation

#ERROR HERE

table(eval, test$output)#essentially the confusion matrix
confusionMatrix(eval, test$output, positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")
#from the above we can see our True Positive Rate or sensitivity is quite good @ 88%, False Positive Rate (1-Specificity) is also not terrible ~ @ 27%, we want this to be low.(Subject to change) 
```

Part 3. Based on your exploration in Part 2, change the threshold using the function provided, what differences do you see in the evaluation metrics? Speak specifically to the metrics you think are best suited to address the questions you are trying to answer. 

```{r echo = FALSE}
#Quick function to explore various threshold levels and output a confusion matrix

adjust_thres <- function(x, y, z) {
  #x=pred_probablities, y=threshold, z=test_outcome
  thres <- as.factor(ifelse(x > y, 1,0))
  confusionMatrix(thres, z, positive = "1", dnn=c("Prediction", "Actual"), mode = "everything")
}

adjust_thres(eval_prob$`1`,.30, test$output) #Not much changes here because of the high probability splits of the data outcomes. Let's take a closer look. We can see that the algo isn't marginally mis-classifying these rows, it's very confidently wrong. Which likely means that there's too much emphasis being placed on too small a number of variables, principally the funfetti variable. 

loan_eval_prob$test <- test$outcome

View(loan_eval_prob)

(error = mean(eval != test$output))#overall error rate, on average when does our prediction not match the actual, looks like around 15%, really just ok.
```

Part 4. Summarize your findings to include recommendations on how you might change each of the two kNN models based on the results. These recommendations might include gathering more data, adjusting the threshold or maybe that it's working fine at the current level and nothing should be done. Regardless of the outcome, what should we be aware of when these models are deployed? 

```{r echo = FALSE}

```